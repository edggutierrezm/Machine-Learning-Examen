import pandas as pd
import numpy as np
import os
import gc # Herramienta esencial para liberar memoria en datasets grandes

# ==========================================================
# üìå CONFIGURACI√ìN DE RUTA Y CARGA DE DATOS
# ==========================================================
# Ajusta esta ruta si tus archivos Parquet no est√°n en una carpeta 'data' al mismo nivel
DATA_PATH = './data/'

def load_parquet_file(filename):
    """Carga un archivo .parquet de forma segura."""
    file_path = os.path.join(DATA_PATH, filename)
    try:
        df = pd.read_parquet(file_path)
        print(f"Cargado: {filename}. Shape: {df.shape}")
        return df
    except Exception as e:
        print(f"ERROR: No se pudo cargar {filename}. {e}")
        return None

# ==========================================================
# üìå UTILITY: FUNCI√ìN DE AGREGACI√ìN GENERAL
# ==========================================================

def aggregate_dataframe(df, group_var, prefix):
    """Calcula agregaciones b√°sicas (mean, max, min, sum, count) para un DF por grupo."""
    
    # 1. Agregaciones Num√©ricas
    # Excluimos la variable de agrupaci√≥n
    num_df = df.select_dtypes(include=np.number).drop(columns=[group_var], errors='ignore')
    agg_num = num_df.groupby(group_var).agg(['mean', 'max', 'min', 'sum', 'count'])
    
    # Aplanar y renombrar las columnas
    agg_num.columns = [prefix + '_' + '_'.join(col).strip().upper() 
                       for col in agg_num.columns.values]
    
    # 2. Agregaciones Categ√≥ricas (Opcional, se puede hacer con One-Hot Encoding del DF original)
    # Aqu√≠ solo nos centramos en las num√©ricas para simplificar el flujo.
    
    return agg_num.reset_index()


# ==========================================================
# üìå FUNCI√ìN CLAVE: INGENIER√çA DE FEATURES DE BUR√ì
# ==========================================================

def get_bureau_features(df_app):
    """
    Procesa bureau.parquet y bureau_balance.parquet.
    Esta es una agregaci√≥n en dos etapas (Multi-nivel).
    """
    
    print("\n--- Iniciando Ingenier√≠a de Bureau ---")
    bureau = load_parquet_file('bureau.parquet')
    bureau_balance = load_parquet_file('bureau_balance.parquet')
    
    # ----------------------------------------------------
    # ETAPA 1: Procesar bureau_balance (Agregaci√≥n por cr√©dito SK_ID_BUREAU)
    # Clave de uni√≥n: SK_ID_BUREAU
    # ----------------------------------------------------
    
    # Ejemplo de ingenier√≠a en bureau_balance (tiempo de atraso y estatus)
    bureau_balance['STATUS_C_COUNT'] = (bureau_balance['STATUS'] == 'C').astype(int)
    
    bb_agg = aggregate_dataframe(bureau_balance, 'SK_ID_BUREAU', 'BB')
    
    # Merge de las agregaciones de balance al DF de bureau
    bureau = bureau.merge(bb_agg, on='SK_ID_BUREAU', how='left')
    
    # Liberar memoria de los DFs intermedios
    del bureau_balance, bb_agg; gc.collect()
    
    # ----------------------------------------------------
    # ETAPA 2: Procesar Bureau (Agregaci√≥n por cliente SK_ID_CURR)
    # Clave de uni√≥n: SK_ID_CURR
    # ----------------------------------------------------
    
    # Ingenier√≠a de Caracter√≠sticas Espec√≠ficas del Bur√≥
    # Ratio de Deuda vs. Cr√©dito Total
    bureau['DEBT_TO_CREDIT_RATIO'] = \
        bureau['AMT_CREDIT_SUM_DEBT'] / (bureau['AMT_CREDIT_SUM'] + 0.0001)
        
    # Agregaci√≥n final por cliente (SK_ID_CURR)
    bureau_agg = aggregate_dataframe(bureau, 'SK_ID_CURR', 'BUREAU')
    
    # ----------------------------------------------------
    # ETAPA 3: Uni√≥n al DataFrame Principal
    # ----------------------------------------------------
    
    df_app = df_app.merge(bureau_agg, on='SK_ID_CURR', how='left')
    
    # Liberar memoria
    del bureau, bureau_agg; gc.collect()
    
    print(f"Bureau Features a√±adidas. DF_App shape: {df_app.shape}")
    return df_app

# ==========================================================
# üìå FUNCI√ìN CLAVE: INGENIER√çA DE FEATURES DE SOLICITUDES PREVIAS
# (Estructura de ejemplo, debes completarla)
# ==========================================================

def get_prev_app_features(df_app):
    """Procesa previous_application.parquet y lo agrega por cliente (SK_ID_CURR)."""
    
    print("\n--- Iniciando Ingenier√≠a de Solicitudes Previas ---")
    prev = load_parquet_file('previous_application.parquet')
    
    # Ingenier√≠a de Features: Ratios importantes
    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_APPLICATION'] / (prev['AMT_ANNUITY'] + 0.0001)
    
    # Agregaci√≥n por cliente (SK_ID_CURR)
    prev_agg = aggregate_dataframe(prev, 'SK_ID_CURR', 'PREV')
    
    # Uni√≥n al DataFrame Principal
    df_app = df_app.merge(prev_agg, on='SK_ID_CURR', how='left')
    
    del prev, prev_agg; gc.collect()
    print(f"Previous App Features a√±adidas. DF_App shape: {df_app.shape}")
    return df_app


# ==========================================================
# üìå FLUJO PRINCIPAL DE INGENIER√çA
# ==========================================================

def run_feature_engineering_pipeline():
    # Cargar la tabla principal
    df_app = load_parquet_file('application_.parquet')
    
    if df_app is None:
        print("No se pudo iniciar el pipeline: La tabla principal es nula.")
        return

    # 1. Integrar Features de Bur√≥
    df_app = get_bureau_features(df_app)
    
    # 2. Integrar Features de Solicitudes Previas
    df_app = get_prev_app_features(df_app)
    
    # 3. Integrar el resto de tablas (Instalments, POS_CASH, Credit_Card...)
    # df_app = get_installments_features(df_app) 
    # df_app = get_credit_card_features(df_app) 

    print("-" * 50)
    print(f"‚úÖ PIPELINE COMPLETADO. Dataset Maestro Final Shape: {df_app.shape}")
    print("-" * 50)
    
    # üíæ Guardar el resultado en artifacts para la siguiente fase
    # Se recomienda guardar el dataframe maestro (con TARGET, SK_ID_CURR, y las nuevas features)
    df_app.to_csv('../artifacts/master_train_features.csv', index=False)
    print("Archivo maestro guardado en artifacts/master_train_features.csv")


if __name__ == "__main__":
    # La ruta de los datos debe estar configurada en DATA_PATH arriba
    run_feature_engineering_pipeline()